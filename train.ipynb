{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from racing_env import RacingEnv\n",
    "from racing_policy import FeatureEncoder, FeatureExtractor\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import numpy as np\n",
    "from constants import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = RacingEnv(gui=False)\n",
    "eval_env = Monitor(env)\n",
    "\n",
    "# Define the reward threshold at which training should stop.\n",
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=REWARD_THRESHOLD, verbose=1)\n",
    "eval_callback = EvalCallback(\n",
    "    env,\n",
    "    best_model_save_path='./logs/best_model/',\n",
    "    log_path='./logs/results/',\n",
    "    eval_freq=4096,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    callback_on_new_best=stop_callback  # Stop training if new best meets threshold.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    device=\"cpu\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    n_steps=N_STEPS,\n",
    "    batch_size=MINI_BATCH_SIZE,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    policy_kwargs=dict(\n",
    "        net_arch=NET_ARCH,\n",
    "        # features_extractor_class=FeatureExtractor,\n",
    "        # activation_fn=lambda: torch.nn.LeakyReLU(negative_slope=0.2),\n",
    "        # optimizer_class=torch.optim.Adam,\n",
    "    ),\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=TOTAL_TIMESTEPS, callback=eval_callback, progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model\n",
    "for episode in range(10):\n",
    "    obs, _ = eval_env.reset()\n",
    "    done = False\n",
    "    truncated = False\n",
    "    total_reward = 0\n",
    "    while not done and not truncated:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, truncated, info = eval_env.step(action)\n",
    "        total_reward += reward\n",
    "        eval_env.render()\n",
    "    print(f\"Episode {episode} Reward: {total_reward:.4f} Info: {info.get('done_reason', 'N/A')}\")\n",
    "eval_env.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
